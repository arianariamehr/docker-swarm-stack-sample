# Swarm manager high availability (HA)  

Swarm managers have native support for high availability (HA). This means one or more can fail, and the survivors will keep the swarm running.  

Technically speaking, swarm implements a form of **active-passive multi-manager HA**. This means that although you have multiple managers, only one of them is active at any given moment. This active manager is called the `leader`, and the leader is the only one that will ever issue live commands against the swarm. So, it’s only ever the leader that changes the config, or issues tasks to workers. If a follower manager (`passive`) receives commands for the swarm, it proxies them across to the leader.  

![image](https://github.com/arianariamehr/docker-swarm-stack-sample/assets/130653489/e52fd76b-f813-4768-a5fe-684fc3eabc5c)  

Step 1 is the command coming in to a manager from a remote Docker client. Step 2 is the non-leader manager receiving the command and proxying it to the leader. Step 3 is the leader executing the command on the swarm.  

**Note**: Notice that managers are either *leaders* or *followers*. This is Raft terminology, because swarm uses an implementation of the **Raft consensus algorithm** to maintain a consistent cluster state across multiple highly available managers.  

On the topic of HA, the following two best practices apply:  
1. Deploy an odd number of managers.
2. Don’t deploy too many managers (3 or 5 is recommended)  

Having an odd number of managers reduces the chances of split-brain conditions. For example, if you had 4 managers and the network partitioned, you could be left with two managers on each side of the partition. This is known as a `split brain` — each side knows there used to be 4 but can now only see 2. But crucially, neither side has any way of knowing if the other two are still alive and whether it holds a majority (`quorum`).  

However, if you have 3 or 5 managers and the same network partition occurs, it is impossible to have an equal number of managers on both sides of the partition. This means that one side achieves quorum and full cluster management services remain available.  

![image](https://github.com/arianariamehr/docker-swarm-stack-sample/assets/130653489/da5ca3e3-a270-442a-acfb-e16eb7a47e0c)

It’s a best practice to have either 3 or 5 managers for HA. 7 might work, but it’s generally accepted that 3 or 5 is optimal. You definitely don’t want more than 7, as the time taken to aieve consensus will be longer.  


### Built-in Swarm security  
Swarm clusters have a ton of built-in security that’s configured out-of-the-box with sensible defaults — CA settings, join tokens, mutual TLS, encrypted cluster store, encrypted networks, cryptographic node ID’s and more.  


### Locking a Swarm  
Despite all of this built-in native security, restarting an older manager or restoring an old backup has the potential to compromise the cluster. Old managers re-joining a swarm automatically decrypt and gain access to the Raft log time-series database — this can pose security concerns. Restoring old bachups can also wipe the current swarm configuration.  

To prevent situations like these, Docker allows you to lock a swarm with the `Autolock feature`. This forces restarted managers to present the cluster unlock key before being admitted back into the cluster.  

It’s possible to apply a lock directly to a new swarm by passing the `--autolock` flag to the `docker swarm init` command. However, we’ve already built a swarm, so we’ll lock our existing swarm with the `docker swarm update` command.  

Run the following command from a swarm manager.  
```
swarm-manager-01:~# docker swarm update --autolock=true
Swarm updated.
To unlock a swarm manager after it restarts, run the `docker swarm unlock`
command and provide the following key:

    SWMKEY-1-LyDlJJBVRwcKfBcdn6Ldg0o9AAOg9ODkhQErtCquqo8

Please remember to store this key in a password manager, since without it you
will not be able to restart the manager.
```  

Be sure to keep the unlock key in a secure place. You can always check your current swarm unlock key with the `docker swarm unlock-key` command.  

Restart one of your manager nodes to see if it automatically re-joins the cluster. You may need to prepend the command with sudo.  
```
swarm-manager-01:~#  service docker restart
```

Try and list the nodes in the swarm.  
```
swarm-manager-01:~# docker node ls
Error response from daemon: Swarm is encrypted and needs to be unlocked before it can be
used. Please use "docker swarm unlock" to unlock it.
```

Although the Docker service has restarted on the manager, it has not been allowed to re-join the swarm. You can prove this even further by running the docker node ls command on another manager node. The restarted manager will show as down and unreachable.  

Use the `docker swarm unlock` command to unlock the swarm for the restarted manager.  
```
swarm-manager-01:~# docker swarm unlock
Please enter unlock key:  <enter your key>
```
The node will be allowed to re-join the swarm and will show as ready and reachable if you run another `docker node ls`.  

**Note**: Locking your swarm and protecting the unlock key is recommended for **production** environments.
